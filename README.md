# Trust in Medical AI: Communicating Uncertainties to Patients
This project contains the data analysis for my Master thesis in Artificial Intelligence at Radboud University.

## Abstract
The adoption of Medical Decision Support Systems (MDSS) faces a tension between regulatory transparency requirements (e.g., EU AI Act) and concerns that communicating epistemic uncertainties might undermine patient trust. Ambiguity aversion theory suggests that acknowledging AI uncertainty could spuriously reduce trust even when systems are reliable. This study investigates whether transparent communication about epistemic uncertainty in MDSS indeed negatively affects people's trust, and explores how demographic and psychological factors influence this relationship.

A between-subjects online experiment (N=255) compared patient information leaflets about an MDSS. The control condition presented standard accuracy information (90\%), while the uncertainty condition additionally communicated epistemic uncertainty with confidence intervals (82\%-98\%). Trust was measured using the Trust in Automation questionnaire across five outcome dimensions. Covariates included age, gender, education, AI experience, healthcare system trust, and technology affinity.

Non-inferiority testing showed that there was no significant negative effect of uncertainty communication on the overall distribution of trust scores. Three of five dimensions demonstrated also non-inferiority (Familiarity, Propensity to Trust, Understanding/Predictability), while two remained inconclusive (Reliability/Competence, Trust in Automation). Age was the strongest predictor (older participants showed higher trust), and AI experience positively predicted trust. No moderation effects were found, indicating consistent effects across demographic groups.

These findings support the distinction between trust (subjective attitude) and trustworthiness (objective property): communicating system properties does not automatically undermine patient attitudes. Healthcare institutions can meet transparency requirements without risking trust erosion. This research contributes to trust theory in human-AI interaction and provides practical guidance for implementing transparency in medical AI systems.
